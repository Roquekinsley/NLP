{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForTokenClassification, DataCollatorForTokenClassification\n",
    "import tensorflow as tf\n",
    "from seqeval.metrics import classification_report\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "import nest_asyncio\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappage des labels simplifié\n",
    "label_map = {\"person\": 1, \"content\": 2, \"O\": 0}\n",
    "# Chargement et prétraitement des données\n",
    "def load_and_preprocess(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['words'] = df['words'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    df['labels'] = df['labels'].apply(lambda x: [label_map.get(label, 0) for label in eval(x)] if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "train = load_and_preprocess(\"data/Data/train_2.csv\")\n",
    "test = load_and_preprocess(\"data/Data/test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Conversion des données en format Dataset\n",
    "data = Dataset.from_pandas(train)\n",
    "test_data = Dataset.from_pandas(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer and label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2931/2931 [00:00<00:00, 12547.39 examples/s]\n",
      "Map: 100%|██████████| 1991/1991 [00:00<00:00, 13935.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du tokeniseur\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenisation et alignement des labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"words\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        labels.append([-100 if word_id is None else label[word_id] for word_id in word_ids])\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_data = data.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test_data = test_data.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 612s 3s/step - loss: 0.1215\n",
      "125/125 [==============================] - 120s 944ms/step - loss: 0.0527\n",
      "Perte sur l'ensemble de test: 0.05273259058594704\n"
     ]
    }
   ],
   "source": [
    "# Création du modèle\n",
    "model = TFDistilBertForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_map))\n",
    "\n",
    "# Préparation des données pour l'entraînement et l'évaluation\n",
    "def prepare_dataset(tokenized_data, batch_size=16, max_length=128):\n",
    "    def align_and_truncate(data):\n",
    "        # Alignement et troncature des séquences et des masques d'attention\n",
    "        input_ids = tf.keras.preprocessing.sequence.pad_sequences(data['input_ids'], maxlen=max_length, dtype='long', padding='post', truncating='post')\n",
    "        attention_mask = tf.keras.preprocessing.sequence.pad_sequences(data['attention_mask'], maxlen=max_length, dtype='long', padding='post', truncating='post')\n",
    "        labels = tf.keras.preprocessing.sequence.pad_sequences(data['labels'], maxlen=max_length, dtype='long', padding='post', truncating='post', value=-100)\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask}, labels\n",
    "\n",
    "    features, labels = align_and_truncate(tokenized_data)\n",
    "    return tf.data.Dataset.from_tensor_slices((features, labels)).shuffle(10000).batch(batch_size)\n",
    "\n",
    "train_dataset = prepare_dataset(tokenized_data)\n",
    "test_dataset = prepare_dataset(tokenized_test_data)\n",
    "\n",
    "# Custom loss pour ignorer les labels -100 lors du calcul de la perte\n",
    "def custom_loss(y_true, y_pred):\n",
    "    active_loss = tf.reshape(y_true, (-1,)) != -100\n",
    "    reduced_logits = tf.boolean_mask(tf.reshape(y_pred, (-1, tf.shape(y_pred)[2])), active_loss)\n",
    "    reduced_labels = tf.boolean_mask(tf.reshape(y_true, (-1,)), active_loss)\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(reduced_labels, reduced_logits, from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=custom_loss)\n",
    "\n",
    "# Entraînement du modèle\n",
    "history = model.fit(train_dataset, epochs=1)\n",
    "\n",
    "# Évaluation du modèle\n",
    "eval_loss = model.evaluate(test_dataset)\n",
    "print(f\"Perte sur l'ensemble de test: {eval_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 958ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 968ms/step\n",
      "1/1 [==============================] - 1s 951ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 981ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 944ms/step\n",
      "1/1 [==============================] - 1s 998ms/step\n",
      "1/1 [==============================] - 1s 986ms/step\n",
      "1/1 [==============================] - 1s 952ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 958ms/step\n",
      "1/1 [==============================] - 1s 995ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 990ms/step\n",
      "1/1 [==============================] - 1s 970ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 965ms/step\n",
      "1/1 [==============================] - 1s 948ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 980ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 956ms/step\n",
      "1/1 [==============================] - 1s 960ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 950ms/step\n",
      "1/1 [==============================] - 1s 955ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 984ms/step\n",
      "1/1 [==============================] - 1s 998ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 940ms/step\n",
      "1/1 [==============================] - 1s 969ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 984ms/step\n",
      "1/1 [==============================] - 1s 997ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 963ms/step\n",
      "1/1 [==============================] - 1s 982ms/step\n",
      "1/1 [==============================] - 1s 982ms/step\n",
      "1/1 [==============================] - 1s 957ms/step\n",
      "1/1 [==============================] - 1s 968ms/step\n",
      "1/1 [==============================] - 1s 1000ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 982ms/step\n",
      "1/1 [==============================] - 1s 989ms/step\n",
      "1/1 [==============================] - 1s 986ms/step\n",
      "1/1 [==============================] - 1s 970ms/step\n",
      "1/1 [==============================] - 1s 943ms/step\n",
      "1/1 [==============================] - 1s 990ms/step\n",
      "1/1 [==============================] - 1s 947ms/step\n",
      "1/1 [==============================] - 1s 942ms/step\n",
      "1/1 [==============================] - 1s 983ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 970ms/step\n",
      "1/1 [==============================] - 1s 934ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 982ms/step\n",
      "1/1 [==============================] - 1s 964ms/step\n",
      "1/1 [==============================] - 1s 949ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 979ms/step\n",
      "1/1 [==============================] - 1s 987ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 935ms/step\n",
      "1/1 [==============================] - 1s 968ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 980ms/step\n",
      "1/1 [==============================] - 1s 960ms/step\n",
      "1/1 [==============================] - 1s 994ms/step\n",
      "1/1 [==============================] - 1s 971ms/step\n",
      "1/1 [==============================] - 1s 991ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 998ms/step\n",
      "1/1 [==============================] - 1s 970ms/step\n",
      "1/1 [==============================] - 1s 987ms/step\n",
      "1/1 [==============================] - 1s 960ms/step\n",
      "1/1 [==============================] - 1s 950ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 980ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 945ms/step\n",
      "1/1 [==============================] - 1s 946ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 952ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 980ms/step\n",
      "1/1 [==============================] - 1s 950ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 980ms/step\n",
      "1/1 [==============================] - 1s 995ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 961ms/step\n",
      "1/1 [==============================] - 1s 972ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 972ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 970ms/step\n",
      "1/1 [==============================] - 1s 975ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 977ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 961ms/step\n",
      "1/1 [==============================] - 1s 979ms/step\n",
      "1/1 [==============================] - 1s 993ms/step\n",
      "1/1 [==============================] - 1s 965ms/step\n",
      "1/1 [==============================] - 1s 942ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 983ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 587ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\louis\\miniconda3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: content seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\louis\\miniconda3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: person seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       erson       0.84      0.94      0.89       478\n",
      "      ontent       0.60      0.56      0.58       149\n",
      "\n",
      "   micro avg       0.79      0.85      0.82       627\n",
      "   macro avg       0.72      0.75      0.74       627\n",
      "weighted avg       0.79      0.85      0.82       627\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "{'job': 'send_message', 'receiver': 'alice', 'content': 'to bring the financial reports to the conference next wednesday [SEP]'}\n"
     ]
    }
   ],
   "source": [
    "# Prédiction et calcul des métriques sur l'ensemble de test\n",
    "label_map_inv = {v: k for k, v in label_map.items()}  # Inversion du mappage des labels\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    out_label_list, preds_list = [], []\n",
    "    for i in range(batch_size):\n",
    "        out_label_list.append([label_map_inv.get(label_ids[i][j], \"O\") for j in range(seq_len) if label_ids[i][j] != -100])\n",
    "        preds_list.append([label_map_inv.get(preds[i][j], \"O\") for j in range(seq_len) if label_ids[i][j] != -100])\n",
    "    return preds_list, out_label_list\n",
    "\n",
    "predictions, true_labels = [], []\n",
    "for batch in test_dataset:\n",
    "    logits = model.predict(batch[0])['logits']\n",
    "    labels = batch[1].numpy()\n",
    "    preds, labels = align_predictions(logits, labels)\n",
    "    predictions.extend(preds)\n",
    "    true_labels.extend(labels)\n",
    "\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Extraction des entités à partir d'une phrase donnée\n",
    "def extract_entities(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
    "    predictions = model.predict([inputs[\"input_ids\"], inputs[\"attention_mask\"]])\n",
    "    predicted_label_indices = np.argmax(predictions.logits, axis=2)[0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].numpy()[0])\n",
    "    labels = [label_map_inv.get(idx, \"O\") for idx in predicted_label_indices]\n",
    "    entities = {\"person\": [], \"content\": []}\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label in entities:\n",
    "            entities[label].append(token)\n",
    "    return entities\n",
    "\n",
    "# Exemple d'utilisation de la fonction extract_entities (test pour voir si ça marche)\n",
    "extracted_entities = extract_entities(\"Please remind Alice to bring the financial reports to the conference next Wednesday.\")\n",
    "\n",
    "result = {\n",
    "    \"job\": \"send_message\",\n",
    "    \"receiver\": \" \".join(extracted_entities[\"person\"]),\n",
    "    \"content\": \" \".join(extracted_entities[\"content\"]),\n",
    "}\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [32204]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8001 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:57003 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:57039 - \"POST /process_sentence/ HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [32204]\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Sentence(BaseModel):\n",
    "    test_sentence: str\n",
    "\n",
    "\n",
    "# Simulez votre fonction d'extraction d'entités ici\n",
    "# Remplacez cette logique par votre véritable fonction d'extraction d'entités\n",
    "def extract_entities(sentence):\n",
    "    # Exemple d'entités extraites. À remplacer par votre extraction réelle.\n",
    "    return {\n",
    "        \"person\": [\"Alice\"],  # Simule l'extraction de noms de personnes\n",
    "        \"content\": [\"financial reports\"]  # Simule l'extraction de contenus\n",
    "    }\n",
    "\n",
    "@app.post(\"/process_sentence/\")\n",
    "async def process_sentence(sentence: Sentence):\n",
    "    extracted_entities = extract_entities(sentence.test_sentence)\n",
    "    \n",
    "    # Formatage de la réponse selon les spécifications\n",
    "    result = {\n",
    "        \"job\": \"send_message\",\n",
    "        \"receiver\": \" \".join(extracted_entities.get(\"person\", [])),\n",
    "        \"content\": \" \".join(extracted_entities.get(\"content\", [])),\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Point d'entrée pour Uvicorn si ce fichier est exécuté directement\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8001)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
