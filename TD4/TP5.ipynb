{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\louis\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForTokenClassification, DataCollatorForTokenClassification\n",
    "import tensorflow as tf\n",
    "from seqeval.metrics import classification_report\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "import nest_asyncio\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappage des labels simplifié\n",
    "label_map = {\"person\": 1, \"content\": 2, \"O\": 0}\n",
    "# Chargement et prétraitement des données\n",
    "def load_and_preprocess(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['words'] = df['words'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    df['labels'] = df['labels'].apply(lambda x: [label_map.get(label, 0) for label in eval(x)] if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "train = load_and_preprocess(\"DATA/train_2.csv\")\n",
    "test = load_and_preprocess(\"DATA/test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Conversion des données en format Dataset\n",
    "data = Dataset.from_pandas(train)\n",
    "test_data = Dataset.from_pandas(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer and label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2931/2931 [00:00<00:00, 21152.34 examples/s]\n",
      "Map: 100%|██████████| 1991/1991 [00:00<00:00, 26826.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du tokeniseur\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenisation et alignement des labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"words\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        labels.append([-100 if word_id is None else label[word_id] for word_id in word_ids])\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_data = data.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test_data = test_data.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 416s 2s/step - loss: 0.1290\n",
      "125/125 [==============================] - 85s 667ms/step - loss: 0.0525\n",
      "Perte sur l'ensemble de test: 0.05251718685030937\n"
     ]
    }
   ],
   "source": [
    "# Création du modèle\n",
    "model = TFDistilBertForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_map))\n",
    "\n",
    "# Préparation des données pour l'entraînement et l'évaluation\n",
    "def prepare_dataset(tokenized_data, batch_size=16, max_length=128):\n",
    "    def align_and_truncate(data):\n",
    "        # Alignement et troncature des séquences et des masques d'attention\n",
    "        input_ids = tf.keras.preprocessing.sequence.pad_sequences(data['input_ids'], maxlen=max_length, dtype='long', padding='post', truncating='post')\n",
    "        attention_mask = tf.keras.preprocessing.sequence.pad_sequences(data['attention_mask'], maxlen=max_length, dtype='long', padding='post', truncating='post')\n",
    "        labels = tf.keras.preprocessing.sequence.pad_sequences(data['labels'], maxlen=max_length, dtype='long', padding='post', truncating='post', value=-100)\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask}, labels\n",
    "\n",
    "    features, labels = align_and_truncate(tokenized_data)\n",
    "    return tf.data.Dataset.from_tensor_slices((features, labels)).shuffle(10000).batch(batch_size)\n",
    "\n",
    "train_dataset = prepare_dataset(tokenized_data)\n",
    "test_dataset = prepare_dataset(tokenized_test_data)\n",
    "\n",
    "# Custom loss pour ignorer les labels -100 lors du calcul de la perte\n",
    "def custom_loss(y_true, y_pred):\n",
    "    active_loss = tf.reshape(y_true, (-1,)) != -100\n",
    "    reduced_logits = tf.boolean_mask(tf.reshape(y_pred, (-1, tf.shape(y_pred)[2])), active_loss)\n",
    "    reduced_labels = tf.boolean_mask(tf.reshape(y_true, (-1,)), active_loss)\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(reduced_labels, reduced_logits, from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=custom_loss)\n",
    "\n",
    "# Entraînement du modèle\n",
    "history = model.fit(train_dataset, epochs=1)\n",
    "\n",
    "# Évaluation du modèle\n",
    "eval_loss = model.evaluate(test_dataset)\n",
    "print(f\"Perte sur l'ensemble de test: {eval_loss}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x000001ADD09B11C0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x000001ADD0823040>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x000001ADD0810E80>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x000001ADD07F0D00>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x000001ADD081FB80>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x000001ADD0825A00>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: Model/named_entityr_ecognition\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model/named_entityr_ecognition\\assets\n"
     ]
    }
   ],
   "source": [
    "# Remplacez 'chemin/vers/sauvegarde_classification' par votre chemin réel\n",
    "model.save('Model/named_entityr_ecognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 670ms/step\n",
      "1/1 [==============================] - 1s 636ms/step\n",
      "1/1 [==============================] - 1s 661ms/step\n",
      "1/1 [==============================] - 1s 635ms/step\n",
      "1/1 [==============================] - 1s 661ms/step\n",
      "1/1 [==============================] - 1s 650ms/step\n",
      "1/1 [==============================] - 1s 638ms/step\n",
      "1/1 [==============================] - 1s 656ms/step\n",
      "1/1 [==============================] - 1s 658ms/step\n",
      "1/1 [==============================] - 1s 650ms/step\n",
      "1/1 [==============================] - 1s 624ms/step\n",
      "1/1 [==============================] - 1s 625ms/step\n",
      "1/1 [==============================] - 1s 624ms/step\n",
      "1/1 [==============================] - 1s 600ms/step\n",
      "1/1 [==============================] - 1s 576ms/step\n",
      "1/1 [==============================] - 1s 606ms/step\n",
      "1/1 [==============================] - 1s 609ms/step\n",
      "1/1 [==============================] - 1s 575ms/step\n",
      "1/1 [==============================] - 1s 588ms/step\n",
      "1/1 [==============================] - 1s 607ms/step\n",
      "1/1 [==============================] - 1s 610ms/step\n",
      "1/1 [==============================] - 1s 665ms/step\n",
      "1/1 [==============================] - 1s 658ms/step\n",
      "1/1 [==============================] - 1s 705ms/step\n",
      "1/1 [==============================] - 1s 662ms/step\n",
      "1/1 [==============================] - 1s 648ms/step\n",
      "1/1 [==============================] - 1s 651ms/step\n",
      "1/1 [==============================] - 1s 656ms/step\n",
      "1/1 [==============================] - 1s 657ms/step\n",
      "1/1 [==============================] - 1s 648ms/step\n",
      "1/1 [==============================] - 1s 644ms/step\n",
      "1/1 [==============================] - 1s 634ms/step\n",
      "1/1 [==============================] - 1s 629ms/step\n",
      "1/1 [==============================] - 1s 632ms/step\n",
      "1/1 [==============================] - 1s 636ms/step\n",
      "1/1 [==============================] - 1s 667ms/step\n",
      "1/1 [==============================] - 1s 663ms/step\n",
      "1/1 [==============================] - 1s 649ms/step\n",
      "1/1 [==============================] - 1s 643ms/step\n",
      "1/1 [==============================] - 1s 646ms/step\n",
      "1/1 [==============================] - 1s 648ms/step\n",
      "1/1 [==============================] - 1s 649ms/step\n",
      "1/1 [==============================] - 1s 646ms/step\n",
      "1/1 [==============================] - 1s 638ms/step\n",
      "1/1 [==============================] - 1s 647ms/step\n",
      "1/1 [==============================] - 1s 629ms/step\n",
      "1/1 [==============================] - 1s 635ms/step\n",
      "1/1 [==============================] - 1s 628ms/step\n",
      "1/1 [==============================] - 1s 636ms/step\n",
      "1/1 [==============================] - 1s 630ms/step\n",
      "1/1 [==============================] - 1s 645ms/step\n",
      "1/1 [==============================] - 1s 695ms/step\n",
      "1/1 [==============================] - 1s 643ms/step\n",
      "1/1 [==============================] - 1s 638ms/step\n",
      "1/1 [==============================] - 1s 632ms/step\n",
      "1/1 [==============================] - 1s 628ms/step\n",
      "1/1 [==============================] - 1s 625ms/step\n",
      "1/1 [==============================] - 1s 636ms/step\n",
      "1/1 [==============================] - 1s 629ms/step\n",
      "1/1 [==============================] - 1s 634ms/step\n",
      "1/1 [==============================] - 1s 629ms/step\n",
      "1/1 [==============================] - 1s 636ms/step\n",
      "1/1 [==============================] - 1s 637ms/step\n",
      "1/1 [==============================] - 1s 624ms/step\n",
      "1/1 [==============================] - 1s 632ms/step\n",
      "1/1 [==============================] - 1s 640ms/step\n",
      "1/1 [==============================] - 1s 625ms/step\n",
      "1/1 [==============================] - 1s 626ms/step\n",
      "1/1 [==============================] - 1s 643ms/step\n",
      "1/1 [==============================] - 1s 631ms/step\n",
      "1/1 [==============================] - 1s 644ms/step\n",
      "1/1 [==============================] - 1s 626ms/step\n",
      "1/1 [==============================] - 1s 641ms/step\n",
      "1/1 [==============================] - 1s 642ms/step\n",
      "1/1 [==============================] - 1s 661ms/step\n",
      "1/1 [==============================] - 1s 629ms/step\n",
      "1/1 [==============================] - 1s 637ms/step\n",
      "1/1 [==============================] - 1s 628ms/step\n",
      "1/1 [==============================] - 1s 656ms/step\n",
      "1/1 [==============================] - 1s 629ms/step\n",
      "1/1 [==============================] - 1s 655ms/step\n",
      "1/1 [==============================] - 1s 635ms/step\n",
      "1/1 [==============================] - 1s 641ms/step\n",
      "1/1 [==============================] - 1s 628ms/step\n",
      "1/1 [==============================] - 1s 638ms/step\n",
      "1/1 [==============================] - 1s 633ms/step\n",
      "1/1 [==============================] - 1s 624ms/step\n",
      "1/1 [==============================] - 1s 627ms/step\n",
      "1/1 [==============================] - 1s 626ms/step\n",
      "1/1 [==============================] - 1s 636ms/step\n",
      "1/1 [==============================] - 1s 631ms/step\n",
      "1/1 [==============================] - 1s 634ms/step\n",
      "1/1 [==============================] - 1s 629ms/step\n",
      "1/1 [==============================] - 1s 632ms/step\n",
      "1/1 [==============================] - 1s 621ms/step\n",
      "1/1 [==============================] - 1s 636ms/step\n",
      "1/1 [==============================] - 1s 636ms/step\n",
      "1/1 [==============================] - 1s 620ms/step\n",
      "1/1 [==============================] - 1s 627ms/step\n",
      "1/1 [==============================] - 1s 637ms/step\n",
      "1/1 [==============================] - 1s 624ms/step\n",
      "1/1 [==============================] - 1s 632ms/step\n",
      "1/1 [==============================] - 1s 625ms/step\n",
      "1/1 [==============================] - 1s 612ms/step\n",
      "1/1 [==============================] - 1s 622ms/step\n",
      "1/1 [==============================] - 1s 637ms/step\n",
      "1/1 [==============================] - 1s 667ms/step\n",
      "1/1 [==============================] - 1s 654ms/step\n",
      "1/1 [==============================] - 1s 620ms/step\n",
      "1/1 [==============================] - 1s 631ms/step\n",
      "1/1 [==============================] - 1s 633ms/step\n",
      "1/1 [==============================] - 1s 666ms/step\n",
      "1/1 [==============================] - 1s 644ms/step\n",
      "1/1 [==============================] - 1s 654ms/step\n",
      "1/1 [==============================] - 1s 661ms/step\n",
      "1/1 [==============================] - 1s 690ms/step\n",
      "1/1 [==============================] - 1s 662ms/step\n",
      "1/1 [==============================] - 1s 631ms/step\n",
      "1/1 [==============================] - 1s 615ms/step\n",
      "1/1 [==============================] - 1s 634ms/step\n",
      "1/1 [==============================] - 1s 645ms/step\n",
      "1/1 [==============================] - 1s 626ms/step\n",
      "1/1 [==============================] - 1s 633ms/step\n",
      "1/1 [==============================] - 0s 301ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\louis\\miniconda3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: person seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\louis\\miniconda3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: content seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       erson       0.89      0.92      0.90       478\n",
      "      ontent       0.41      0.56      0.47       149\n",
      "\n",
      "   micro avg       0.75      0.84      0.79       627\n",
      "   macro avg       0.65      0.74      0.69       627\n",
      "weighted avg       0.77      0.84      0.80       627\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "{'job': 'send_message', 'receiver': 'alice', 'content': 'to bring the financial reports to the conference wednesday [SEP]'}\n"
     ]
    }
   ],
   "source": [
    "# Prédiction et calcul des métriques sur l'ensemble de test\n",
    "label_map_inv = {v: k for k, v in label_map.items()}  # Inversion du mappage des labels\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    out_label_list, preds_list = [], []\n",
    "    for i in range(batch_size):\n",
    "        out_label_list.append([label_map_inv.get(label_ids[i][j], \"O\") for j in range(seq_len) if label_ids[i][j] != -100])\n",
    "        preds_list.append([label_map_inv.get(preds[i][j], \"O\") for j in range(seq_len) if label_ids[i][j] != -100])\n",
    "    return preds_list, out_label_list\n",
    "\n",
    "predictions, true_labels = [], []\n",
    "for batch in test_dataset:\n",
    "    logits = model.predict(batch[0])['logits']\n",
    "    labels = batch[1].numpy()\n",
    "    preds, labels = align_predictions(logits, labels)\n",
    "    predictions.extend(preds)\n",
    "    true_labels.extend(labels)\n",
    "\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Extraction des entités à partir d'une phrase donnée\n",
    "def extract_entities(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
    "    predictions = model.predict([inputs[\"input_ids\"], inputs[\"attention_mask\"]])\n",
    "    predicted_label_indices = np.argmax(predictions.logits, axis=2)[0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].numpy()[0])\n",
    "    labels = [label_map_inv.get(idx, \"O\") for idx in predicted_label_indices]\n",
    "    entities = {\"person\": [], \"content\": []}\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label in entities:\n",
    "            entities[label].append(token)\n",
    "    return entities\n",
    "\n",
    "# Exemple d'utilisation de la fonction extract_entities (test pour voir si ça marche)\n",
    "extracted_entities = extract_entities(\"Please remind Alice to bring the financial reports to the conference next Wednesday.\")\n",
    "\n",
    "result = {\n",
    "    \"job\": \"send_message\",\n",
    "    \"receiver\": \" \".join(extracted_entities[\"person\"]),\n",
    "    \"content\": \" \".join(extracted_entities[\"content\"]),\n",
    "}\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP5\n",
    "\n",
    "Partie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question    label_text  label\n",
      "0  What are the recommended prerequisites for the...  question_rag      0\n",
      "1  Does the cybersecurity course cover intrusion ...  question_rag      0\n",
      "2             How can I enroll in the Python course?  question_rag      0\n",
      "3  What are the main basic concepts covered in th...  question_rag      0\n",
      "4  Does the React course include practical projec...  question_rag      0\n",
      "{'input_ids': tensor([[  101,  2054,  2024,  1996,  6749,  3653,  2890, 24871,  2015,  2005,\n",
      "          1996,  4955,  2000,  3698,  4083,  2607,  1029,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2515,  1996, 16941,  3366, 10841, 15780,  2607,  3104, 24554,\n",
      "         10788,  4725,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2129,  2064,  1045, 25612,  1999,  1996, 18750,  2607,  1029,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2054,  2024,  1996,  2364,  3937,  8474,  3139,  1999,  1996,\n",
      "         29296,  2607,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2515,  1996, 10509,  2607,  2421,  6742,  3934,  2000,  6611,\n",
      "          1996,  4342,  8474,  1029,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialisation du tokenizer de DistilBert\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "data = pd.read_csv('DATA/question_classif.csv')\n",
    "print(data.head())\n",
    "\n",
    "# Tokenisation des questions\n",
    "inputs = tokenizer(list(data['question']), padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "print(inputs[:5])\n",
    "\n",
    "# Préparation des labels\n",
    "labels = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class CustomDistilBert(nn.Module):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(CustomDistilBert, self).__init__()\n",
    "        self.distilbert = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
    "        # Vous pouvez ajouter ou modifier des couches ici si nécessaire\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return output\n",
    "    \n",
    "    def predict(self, input_ids, attention_mask=None):\n",
    "        self.eval()  # mettre le modèle en mode évaluation\n",
    "        with torch.no_grad():\n",
    "            outputs = self(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "        return predictions\n",
    "# Initialisation du modèle adapté\n",
    "model = CustomDistilBert(num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "\n",
    "# Préparation des données avant la division\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "train_input_ids, test_input_ids, train_attention_mask, test_attention_mask, train_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_mask, labels_tensor, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Correction dans la création des TensorDatasets\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
    "\n",
    "# Création des DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomDistilBert(\n",
       "  (distilbert): DistilBertForSequenceClassification(\n",
       "    (distilbert): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define device as CPU since CUDA is not available on Mac M1\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Before starting training, make sure to move your model to the chosen device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\louis\\miniconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Training loss: 0.5451419472694397\n",
      "Accuracy: 1.0\n",
      "Epoch 2/20\n",
      "Training loss: 0.21126106530427932\n",
      "Accuracy: 1.0\n",
      "Epoch 3/20\n",
      "Training loss: 0.06777917966246605\n",
      "Accuracy: 1.0\n",
      "Epoch 4/20\n",
      "Training loss: 0.02876548394560814\n",
      "Accuracy: 1.0\n",
      "Epoch 5/20\n",
      "Training loss: 0.015053039230406285\n",
      "Accuracy: 1.0\n",
      "Epoch 6/20\n",
      "Training loss: 0.009596196562051773\n",
      "Accuracy: 1.0\n",
      "Epoch 7/20\n",
      "Training loss: 0.006164871156215668\n",
      "Accuracy: 1.0\n",
      "Epoch 8/20\n",
      "Training loss: 0.00478506488725543\n",
      "Accuracy: 1.0\n",
      "Epoch 9/20\n",
      "Training loss: 0.003982764249667525\n",
      "Accuracy: 1.0\n",
      "Epoch 10/20\n",
      "Training loss: 0.0031928871758282185\n",
      "Accuracy: 1.0\n",
      "Epoch 11/20\n",
      "Training loss: 0.0027028200682252647\n",
      "Accuracy: 1.0\n",
      "Epoch 12/20\n",
      "Training loss: 0.002308120997622609\n",
      "Accuracy: 1.0\n",
      "Epoch 13/20\n",
      "Training loss: 0.002013955032452941\n",
      "Accuracy: 1.0\n",
      "Epoch 14/20\n",
      "Training loss: 0.0019103370839729905\n",
      "Accuracy: 1.0\n",
      "Epoch 15/20\n",
      "Training loss: 0.0016954287653788923\n",
      "Accuracy: 1.0\n",
      "Epoch 16/20\n",
      "Training loss: 0.0015026237815618515\n",
      "Accuracy: 1.0\n",
      "Epoch 17/20\n",
      "Training loss: 0.0013770973077043892\n",
      "Accuracy: 1.0\n",
      "Epoch 18/20\n",
      "Training loss: 0.0013005087850615383\n",
      "Accuracy: 1.0\n",
      "Epoch 19/20\n",
      "Training loss: 0.0011064357357099652\n",
      "Accuracy: 1.0\n",
      "Epoch 20/20\n",
      "Training loss: 0.0011148381046950818\n",
      "Accuracy: 1.0\n",
      "Prédiction : 1\n",
      "Prédiction : 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "\n",
    "# Configuration de l'optimiseur\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Fonction pour l'entraînement\n",
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = [b.to(device) for b in batch]  # Envoyer le batch au device (GPU ou CPU)\n",
    "        inputs, masks, labels = batch\n",
    "        model.zero_grad()  # Réinitialiser les gradients\n",
    "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()  # Calcul des gradients\n",
    "        optimizer.step()  # Mise à jour des poids\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training loss: {average_loss}\")\n",
    "\n",
    "# Fonction pour l'évaluation\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    for batch in test_loader:\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        inputs, masks, labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, attention_mask=masks)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        total_eval_accuracy += (predictions == labels).sum().item()\n",
    "    avg_accuracy = total_eval_accuracy / len(test_loader.dataset)\n",
    "    print(f\"Accuracy: {avg_accuracy}\")\n",
    "\n",
    "# Exécution de l'entraînement et de l'évaluation\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train(model, train_loader, optimizer)\n",
    "    evaluate(model, test_loader)\n",
    "\n",
    "# Après chaque époque, vous pouvez appeler `evaluate(model, test_loader)` pour obtenir l'accuracy sur l'ensemble de test\n",
    "\n",
    "# Pour afficher une prédiction :\n",
    "model.eval()  # Met le modèle en mode évaluation\n",
    "with torch.no_grad():\n",
    "    # Exemple de question tokenisée\n",
    "    sample_question = \"Ask the python teacher where is next class\"\n",
    "    inputs = tokenizer(sample_question, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    outputs = model(input_ids, attention_mask)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1)\n",
    "    \n",
    "    print(f\"Prédiction : {prediction.item()}\")  # Doit afficher 1\n",
    "\n",
    "    sample_question = \"What are the pre-requisite for the python class?\"\n",
    "    inputs = tokenizer(sample_question, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    outputs = model(input_ids, attention_mask)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1)\n",
    "    \n",
    "    print(f\"Prédiction : {prediction.item()}\")  # Doit afficher 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"Model/text_classification.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send_message\n",
      "question_rag\n"
     ]
    }
   ],
   "source": [
    "def classify_task(text):\n",
    "    model.eval()  # Assurez-vous que le modèle est en mode évaluation\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    with torch.no_grad():  # Désactive le calcul du gradient pour accélérer cette opération\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs[0]\n",
    "    prediction = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # Déterminer la tâche basée sur la prédiction\n",
    "    if prediction == 0:\n",
    "        return \"question_rag\"\n",
    "    else:\n",
    "        return \"send_message\"\n",
    "\n",
    "# Exemple d'utilisation\n",
    "print(classify_task(\"Ask the python teacher where is next class\"))\n",
    "print(classify_task(\"What are the pre-requisites for the python class?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsked RAG: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe RAG replied: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHere is the information you requested.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Test de la pipeline\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msend_virtual_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAsk the python teacher where is next class\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(send_virtual_assistant(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the pre-requisites for the python class?\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[30], line 7\u001b[0m, in \u001b[0;36msend_virtual_assistant\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Étape 2: Extraction des entités (si nécessaire)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msend_message\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 7\u001b[0m     entities \u001b[38;5;241m=\u001b[39m \u001b[43mextract_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     receiver \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(entities\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m\"\u001b[39m, []))  \u001b[38;5;66;03m# Exemple d'extraction du destinataire\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     message \u001b[38;5;241m=\u001b[39m user_input  \u001b[38;5;66;03m# Utiliser l'entrée utilisateur directement ou extraire un message spécifique\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m, in \u001b[0;36mextract_entities\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_entities\u001b[39m(sentence):\n\u001b[0;32m     25\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     predicted_label_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions\u001b[38;5;241m.\u001b[39mlogits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     28\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[24], line 18\u001b[0m, in \u001b[0;36mCustomDistilBert.predict\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# mettre le modèle en mode évaluation\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 18\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     20\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\louis\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\louis\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m, in \u001b[0;36mCustomDistilBert.forward\u001b[1;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 12\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\louis\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\louis\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\louis\\miniconda3\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:779\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    777\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 779\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    789\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\louis\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\louis\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\louis\\miniconda3\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:582\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 582\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn_if_padding_and_no_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\louis\\miniconda3\\lib\\site-packages\\transformers\\modeling_utils.py:4116\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m   4113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   4115\u001b[0m \u001b[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[1;32m-> 4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[0;32m   4117\u001b[0m     warn_string \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4121\u001b[0m     )\n\u001b[0;32m   4123\u001b[0m     \u001b[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[0;32m   4124\u001b[0m     \u001b[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "def send_virtual_assistant(user_input):\n",
    "    # Étape 1: Classifier la tâche\n",
    "    task = classify_task(user_input)\n",
    "    \n",
    "    # Étape 2: Extraction des entités (si nécessaire)\n",
    "    if task == \"send_message\":\n",
    "        entities = extract_entities(user_input)\n",
    "        receiver = \" \".join(entities.get(\"person\", []))  # Exemple d'extraction du destinataire\n",
    "        message = user_input  # Utiliser l'entrée utilisateur directement ou extraire un message spécifique\n",
    "        # Étape 3: Exécution de la tâche\n",
    "        response = send_message(receiver, message)\n",
    "    elif task == \"question_rag\":\n",
    "        # Pour question_rag, l'entrée utilisateur est la question\n",
    "        question = user_input\n",
    "        # Étape 3: Exécution de la tâche\n",
    "        response = ask_RAG(question)\n",
    "    else:\n",
    "        response = \"Sorry, I couldn't understand your request.\"\n",
    "    \n",
    "    # Étape 4: Réponse à l'utilisateur\n",
    "    return response\n",
    "\n",
    "# Simulations des fonctions send_message et ask_RAG pour le test\n",
    "def send_message(receiver, message):\n",
    "    return f'Message sent to {receiver}: \"{message}\"'\n",
    "\n",
    "def ask_RAG(question):\n",
    "    return f'Asked RAG: \"{question}\"\\nThe RAG replied: \"Here is the information you requested.\"'\n",
    "\n",
    "# Test de la pipeline\n",
    "print(send_virtual_assistant(\"Ask the python teacher where is next class\"))\n",
    "print(send_virtual_assistant(\"What are the pre-requisites for the python class?\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
